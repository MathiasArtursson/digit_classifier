{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coral-settle",
   "metadata": {},
   "source": [
    "# Digit classifier with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-radical",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-zealand",
   "metadata": {},
   "source": [
    "*%matplotlib inline* allows for plots inside the notebook.\n",
    "Edge items for the print options for tensors are limited, which prints less information for tensors.\n",
    "A manual seed is also set for pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "confidential-shame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe72f135930>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bracelet",
   "metadata": {},
   "source": [
    "Training device is set, which is either the cpu (slower) or gpu (faster). *torch.cuda.is_available()* evaluates if a compatible gpu is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "revised-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(\"Training device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-geography",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-wilson",
   "metadata": {},
   "source": [
    "Datasets are downloaded into a new folder in the current directory, if not already found there. Samples are stored as tuples, where images are tensors and labels are integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "verbal-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "data_path = data_path = os.getcwd() + '/dataset'\n",
    "\n",
    "tensor_mnist = datasets.MNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "tensor_mnist_val = datasets.MNIST(data_path, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-peace",
   "metadata": {},
   "source": [
    "Information for the datasets and samples are presented. Note that the axes are needed to be changed from C × H × W to H × W × C to allow for plots. This is done with *permute*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "final-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 60000\n",
      "Number of samples in validation set: 10000\n",
      "Sample shape: torch.Size([1, 28, 28])\n",
      "Pixel data type: torch.float32\n",
      "Minimum pixel value: 0.0\n",
      "Maximum pixel value: 1.0\n",
      "\n",
      "Example with label = 5:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_t, label = tensor_mnist[0]\n",
    "\n",
    "print(\"Number of samples in training set: {}\".format(len(tensor_mnist)),\n",
    "      \"Number of samples in validation set: {}\".format(len(tensor_mnist_val)),\n",
    "      \"Sample shape: {}\".format(img_t.shape),\n",
    "      \"Pixel data type: {}\".format(img_t.dtype),\n",
    "      \"Minimum pixel value: {}\".format(img_t.min()),\n",
    "      \"Maximum pixel value: {}\".format(img_t.max()),\n",
    "      sep = '\\n')\n",
    "\n",
    "print(\"\\nExample with label = {}:\".format(label))\n",
    "# Axes are changed from C × H × W to H × W × C to allow for plots.\n",
    "plt.imshow(img_t.permute(1, 2, 0)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-permission",
   "metadata": {},
   "source": [
    "Data is normalized to ease training further on. This is done as follows:\n",
    "$$\\tilde{x}^i_j = \\frac{x^i_j - \\mu_j}{\\sigma_j}$$\n",
    ", where $\\tilde{x}^i_j$ is the normalized data from sample $i$ and feature $j$, $x^i_j$ the raw data from sample $i$ and feature $j$, $\\mu_j$ the mean over all samples for feature $j$, and $\\sigma_j$ the standard deviation over all sampltes for feature $j$.\n",
    "\n",
    "The mean and standard deviation for all samples are computed by stacking them.\n",
    "\n",
    "The stacked tensor is unrolled using *view*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "reasonable-interstate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked data: torch.Size([1, 28, 28, 60000])\n",
      "Mean of all pixels: 0.1307\n",
      "Standard deviation of all pixels: 0.3081\n"
     ]
    }
   ],
   "source": [
    "# Stack samples\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_mnist], dim=3)\n",
    "print(\"Stacked data: {}\".format(imgs.shape))\n",
    "\n",
    "\n",
    "# view(1, -1) keeps the first channel and merges all the remaining dimensions into one. \n",
    "# Our 1 × 28 × 28 x 60000 images are transformed into a 1 x (28*28*60000) tensor.\n",
    "imgs_mean = imgs.view(1, -1).mean(dim=1)\n",
    "imgs_std = imgs.view(1, -1).std(dim=1)\n",
    "\n",
    "print(\"Mean of all pixels: {:0.4f}\".format(imgs_mean[0]),\n",
    "      \"Standard deviation of all pixels: {:0.4f}\".format(imgs_std[0]),\n",
    "      sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-diversity",
   "metadata": {},
   "source": [
    "Normalized datasets are fetched using the computed mean and standard deviation. \n",
    "\n",
    "Note that the validation set is normalized with the training set mean and standard deviation. This is because the model will be shaped by the training set. The validation set must therefore have the same data preproccessing, so that the model will recognize validation set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "green-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mnist = datasets.MNIST(data_path, train=True, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(imgs_mean, imgs_std)]))\n",
    "\n",
    "normalized_mnist_val = datasets.MNIST(data_path, train=False, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(imgs_mean, imgs_std)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-solomon",
   "metadata": {},
   "source": [
    "The datasets are prepared for training and evaluation by being put in data loaders. This makes the data compatible with forward- and backpropagation methods. \n",
    "\n",
    "The data is also divided into batches. This is made because the training set is very large, and using the whole dataset for forward- and backpropagation would be very computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "skilled-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Help on MNIST in module torchvision.datasets.mnist object:\n",
      "\n",
      "class MNIST(torchvision.datasets.vision.VisionDataset)\n",
      " |  MNIST(*args, **kwds)\n",
      " |  \n",
      " |  `MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
      " |          and  ``MNIST/processed/test.pt`` exist.\n",
      " |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      " |          otherwise from ``test.pt``.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MNIST\n",
      " |      torchvision.datasets.vision.VisionDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  download(self) -> None\n",
      " |      Download the MNIST data if it doesn't exist in processed_folder already.\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  class_to_idx\n",
      " |  \n",
      " |  processed_folder\n",
      " |  \n",
      " |  raw_folder\n",
      " |  \n",
      " |  test_data\n",
      " |  \n",
      " |  test_labels\n",
      " |  \n",
      " |  train_data\n",
      " |  \n",
      " |  train_labels\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...\n",
      " |  \n",
      " |  resources = [('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyt...\n",
      " |  \n",
      " |  test_file = 'test.pt'\n",
      " |  \n",
      " |  training_file = 'training.pt'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(normalized_mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#for imgs_val, labels_val in val_loader:\n",
    "#    print(\"Batch of validation images: {}\".format(imgs_val.shape),\n",
    "#          \"Batch of validation labels: {}\".format(labels_val.shape),\n",
    "#          sep = \"\\n\")\n",
    "#    break\n",
    "\n",
    "print(len(normalized_mnist))\n",
    "help(normalized_mnist)\n",
    "#train_subset = normalized_mnist_val[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-photography",
   "metadata": {},
   "source": [
    "Large obsolete objects are deleted from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "imperial-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imgs\n",
    "del tensor_mnist\n",
    "del tensor_mnist_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-vienna",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-tennis",
   "metadata": {},
   "source": [
    "A neural network is initialized with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "finite-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize linear neural network model with one hidden layer. \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "hidden_features = 250\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, hidden_features),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(hidden_features, 10),\n",
    "    nn.LogSoftmax(dim=1))          # Converts output vector first to proabilities by\n",
    "                                   # applying function torch.exp(x) / torch.exp(x).sum()\n",
    "                                   # It then takes the log of this. We need the natural logarithmic\n",
    "                                   # expression for the cost later on, and doing the \n",
    "                                   # calculation of ln in the model helps stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "humanitarian-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw image shape: torch.Size([1, 28, 28]), Raw image stride: (1, 28, 1)\n",
      "Unrolled image shape: torch.Size([1, 784]), Unrolled image stride: (784, 1)\n",
      "Label tensor shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Shaping input data\n",
    "# This is done by unrolling the 28x28 image into a 1D vector.\n",
    "# Unrolling is done through \"view\", which changes the stride \n",
    "# of the contiguous linear container of numbers in memory.\n",
    "# view(-1) changes the stride so that the resulting shape becomes\n",
    "# torch.Size([a]), where \"a\" is \"however many indexes are\n",
    "# left, given the other dimensions and the original number of elements.”\n",
    "\n",
    "img, label = normalized_mnist[0]\n",
    "img_unrolled = img.view(-1).unsqueeze(0)    # Cost function requires the first dimension to be the batch, hence we need to add one dimension with unsqueeze \n",
    "label_tensor = torch.tensor([label])\n",
    "\n",
    "print(\"Raw image shape: {}, Raw image stride: {}\".format(img.shape, img.stride()),\n",
    "     \"Unrolled image shape: {}, Unrolled image stride: {}\".format(img_unrolled.shape, img_unrolled.stride()),\n",
    "     \"Label tensor shape: {}\".format(label_tensor.shape),\n",
    "     sep = \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "herbal-taste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Output from model: tensor([[-2.5628, -2.4362, -2.0545, -1.8584, -2.0896, -2.6426, -2.1723, -2.1898,\n",
      "         -2.7706, -2.6892]], grad_fn=<LogSoftmaxBackward>)\n",
      "Hypothesis probabilities from model: tensor([[0.0771, 0.0875, 0.1282, 0.1559, 0.1237, 0.0712, 0.1139, 0.1119, 0.0626,\n",
      "         0.0679]])\n",
      "The largest probability from the model is 0.1559, i.e. the predicted label is 3.\n"
     ]
    }
   ],
   "source": [
    "# Invoke model\n",
    "import math\n",
    "\n",
    "out = model(img_unrolled)    # natural logarithm of hypothesis probabilities from model \n",
    "out_probabilities = math.e ** out.detach()\n",
    "largest_probability, predicted_label = torch.max(out_probabilities, dim=1)\n",
    "print(\"Label: {}\".format(label),\n",
    "      \"Output from model: {}\".format(out),\n",
    "      \"Hypothesis probabilities from model: {}\".format(out_probabilities),\n",
    "      \"The largest probability from the model is {:0.4f}, i.e. the predicted label is {}.\".format(largest_probability[0], predicted_label[0]),\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "adjusted-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate loss function\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "amazing-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6426, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate loss for test image\n",
    "loss_fn(out, label_tensor) #Calculates sum(-out[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "institutional-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Prepare training set sizes for learning curves\n",
    "training_set_sizes = torch.tensor([1200, 2400, 6000, 20000, 30000, 40000, 50000, 60000])\n",
    "\n",
    "\n",
    "\n",
    "trainset_1 = torch.utils.data.Subset(normalized_mnist, list(range(0, 16)))\n",
    "print(len(trainset_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "generic-affairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1200, Epoch: 0, Epoch time: 1.6 s, Average epoch training loss: 0.0060\n",
      "Training set size: 1200, Epoch: 1, Epoch time: 1.2 s, Average epoch training loss: 0.0036\n",
      "Training set size: 1200, Epoch: 2, Epoch time: 1.0 s, Average epoch training loss: 0.0030\n",
      "Training set size: 1200, Epoch: 3, Epoch time: 1.0 s, Average epoch training loss: 0.0026\n",
      "Training set size: 1200, Epoch: 4, Epoch time: 1.0 s, Average epoch training loss: 0.0023\n",
      "Training set size: 1200, Epoch: 5, Epoch time: 1.0 s, Average epoch training loss: 0.0020\n",
      "Loss_sum_train: 0.1387304961681366, Train loader length: 75\n",
      "Loss_sum_val: 95.93080139160156, Val loader length: 625\n",
      "Model training complete. Training set size: 1200, Training time: 15.3 s, Training set loss: 0.0018, Validation set loss: 0.1535\n",
      "Training set size: 2400, Epoch: 0, Epoch time: 2.0 s, Average epoch training loss: 0.0047\n",
      "Training set size: 2400, Epoch: 1, Epoch time: 2.0 s, Average epoch training loss: 0.0027\n",
      "Training set size: 2400, Epoch: 2, Epoch time: 2.1 s, Average epoch training loss: 0.0021\n",
      "Training set size: 2400, Epoch: 3, Epoch time: 2.0 s, Average epoch training loss: 0.0018\n",
      "Training set size: 2400, Epoch: 4, Epoch time: 2.0 s, Average epoch training loss: 0.0017\n",
      "Training set size: 2400, Epoch: 5, Epoch time: 2.0 s, Average epoch training loss: 0.0015\n",
      "Loss_sum_train: 0.20981241762638092, Train loader length: 150\n",
      "Loss_sum_val: 95.47142791748047, Val loader length: 625\n",
      "Model training complete. Training set size: 2400, Training time: 20.1 s, Training set loss: 0.0014, Validation set loss: 0.1528\n",
      "Training set size: 6000, Epoch: 0, Epoch time: 4.9 s, Average epoch training loss: 0.0217\n",
      "Training set size: 6000, Epoch: 1, Epoch time: 5.1 s, Average epoch training loss: 0.0099\n",
      "Training set size: 6000, Epoch: 2, Epoch time: 5.0 s, Average epoch training loss: 0.0049\n",
      "Training set size: 6000, Epoch: 3, Epoch time: 6.4 s, Average epoch training loss: 0.0028\n",
      "Training set size: 6000, Epoch: 4, Epoch time: 6.1 s, Average epoch training loss: 0.0020\n",
      "Training set size: 6000, Epoch: 5, Epoch time: 4.9 s, Average epoch training loss: 0.0018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-56d8478f536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss_sum_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mimgs_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Move tensor to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Move tensor to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input tensor should be a torch tensor. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         raise ValueError('Expected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = '\n\u001b[1;32m    270\u001b[0m                          '{}.'.format(tensor.size()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loops\n",
    "import torch.optim as optim\n",
    "import timeit\n",
    "\n",
    "n_epochs = 6\n",
    "learning_rate = 1\n",
    "\n",
    "\n",
    "set_size_loss_tensor_train = torch.zeros(len(training_set_sizes))\n",
    "set_size_loss_tensor_val = torch.zeros(len(training_set_sizes))\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for i, training_set_size in enumerate(training_set_sizes):\n",
    "    tic1 = timeit.default_timer() #Take start time measurement\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(normalized_mnist, list(range(0, training_set_size)))\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    model.__init__ # Reinitialize model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate) #Prepare optimizer\n",
    "\n",
    "\n",
    "    epoch_loss_tensor_train = torch.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        tic2 = timeit.default_timer() #Take start time measurement\n",
    "        \n",
    "        batch_loss_sum_train = 0\n",
    "        for imgs_train, labels_train in train_loader:\n",
    "            imgs_train = imgs_train.to(device=device) #Move tensor to device \n",
    "            labels_train = labels_train.to(device=device) #Move tensor to device\n",
    "\n",
    "            outputs_train = model(imgs_train.view(batch_size, -1)) #Unrolls images to a tensor with size [batch_size, 784]\n",
    "\n",
    "            loss_train = loss_fn(outputs_train, labels_train)\n",
    "            batch_loss_sum_train += loss_train.detach()             # detach removes gradient tracking\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss_tensor_train[epoch] = batch_loss_sum_train/len(train_loader)\n",
    "\n",
    "        toc2 = timeit.default_timer() #Take stop time measurement\n",
    "\n",
    "        print(\"Training set size: {}, Epoch: {}, Epoch time: {:0.1f} s, Average epoch training loss: {:0.4f}\".format(training_set_size, epoch, toc2-tic2, float(epoch_loss_tensor_train[epoch])))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Forward propagation\n",
    "    with torch.no_grad(): # Forward propagation does not accumulate gradients\n",
    "\n",
    "        loss_sum_train = 0\n",
    "        loss_sum_val = 0\n",
    "        \n",
    "        for imgs_train, labels_train in train_loader:\n",
    "            imgs_train = imgs_train.to(device=device) #Move tensor to device \n",
    "            labels_train = labels_train.to(device=device) #Move tensor to device\n",
    "\n",
    "            outputs_train = model(imgs_train.view(batch_size, -1))\n",
    "            loss_sum_train += loss_fn(outputs_train, labels_train).detach()\n",
    "\n",
    "        for imgs_val, labels_val in val_loader:\n",
    "            imgs_val = imgs_val.to(device=device) #Move tensor to device \n",
    "            labels_val = labels_val.to(device=device) #Move tensor to device\n",
    "\n",
    "            outputs_val = model(imgs_val.view(batch_size, -1))\n",
    "            loss_sum_val += loss_fn(outputs_val, labels_val).detach()\n",
    "    \n",
    "    print(\"Loss_sum_train: {}, Train loader length: {}\".format(loss_sum_train, len(train_loader)))\n",
    "    print(\"Loss_sum_val: {}, Val loader length: {}\".format(loss_sum_val, len(val_loader)))\n",
    "    set_size_loss_tensor_train[i] = loss_sum_train/len(train_loader) # For plotting learning curves\n",
    "    set_size_loss_tensor_val[i] = loss_sum_val/len(val_loader) # For plotting learning curves\n",
    "    \n",
    "    toc1 = timeit.default_timer() #Take start time measurement\n",
    "    \n",
    "    print(\"Model training complete. Training set size: {}, Training time: {:0.1f} s, Training set loss: {:0.4f}, Validation set loss: {:0.4f}\".format(training_set_size, toc1-tic1, set_size_loss_tensor_train[i], set_size_loss_tensor_val[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "capital-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAp80lEQVR4nO3de3xV5Zn3/883gXA+E1BJOMdatAoawUOLgodSa5W2WoFabeszdKqordODnf5m5jdOnd/T2qf6VB0rttXqiNbDONLpQa3gWQ5BAQUVEYEERSJnyplcvz/2Cm5DgB3Izk52vu/Xa7+y170OudarNRfXfa9134oIzMzMMlWQ6wDMzKxlceIwM7MGceIwM7MGceIwM7MGceIwM7MGaZPrAJpC7969Y+DAgbkOw8ysRZk3b96HEVFct71VJI6BAwdSUVGR6zDMzFoUSSvqa3dXlZmZNYgTh5mZNYgTh5mZNUhWE4ekcZLekrRU0vX17L9O0mJJCyU9LWlA0j5G0vy0z3ZJ45N990h6N23f8Gzeg5mZfVzWBsclFQK3A+cAVcBcSdMjYnHaYa8C5RGxVdK3gZ8Bl0TETGB4cp2ewFLgybTzvh8Rj2QrdjMz279sVhwjgaURsSwidgIPAhemHxARMyNia7I5Cyip5zoXAX9OO87MzHIom4mjH1CZtl2VtO3PFcCf62mfADxQp+3GpHvrZkntDi9MMzNriGYxOC7pUqAcuKlO+5HAp4An0pp/BBwDnAz0BH64n2tOllQhqaK6uvqQ4vrL6+8zbfbKQzrXzCxfZTNxrAJK07ZLkraPkXQ28GPggojYUWf3V4DHImJXbUNEvB8pO4C7SXWJ7SMipkZEeUSUFxfv8+JjRh6f/x7//qc32LB15yGdb2aWj7KZOOYCZZIGSSoi1eU0Pf0ASSOAO0kljTX1XGMidbqpkioESQLGA683fugp15xVxpYdu/nNC+9m61eYmbU4WUscEbEbmEKqm+kN4KGIWCTpBkkXJIfdBHQGHk4erd2bWCQNJFWxPFvn0vdLeg14DegN/CRb9/DJI7ty3qeO4O4Xl7vqMDNLqDUsHVteXh6HOlfVm6s3Me6W57l67FD+4dxPNHJkZmbNl6R5EVFet71ZDI43Z8cc4arDzCydE0cGrj3raLbs2M2vn/dYh5mZE0cGPnFEFz7/qSO556XlrP+bqw4za92cODJU+4TVr19YlutQzMxyyokjQ3urjhdddZhZ6+bE0QDXnFXG1l17XHWYWavmxNEAnziiC+e56jCzVs6Jo4GuTaqOu5531WFmrZMTRwMd3Tc11vG7l5azzlWHmbVCThyHYO9Yh6sOM2uFnDgOgasOM2vNnDgOkcc6zKy1cuI4RGV9u3D+8Ue56jCzVseJ4zBcM3Yo23btYepzrjrMrPVw4jgMtVXHvS8vZ+2WuosXmpnlJyeOw3TtWamq4y7PnGtmrYQTx2Ea2qcLX3DVYWatSFYTh6Rxkt6StFTS9fXsv07SYkkLJT0taUDavj3JcrJ1l5QdJGl2cs3fJ+uZ59Q1SdUx1U9YmVkrkLXEIakQuB34HDAMmChpWJ3DXgXKI+J44BHgZ2n7tkXE8ORzQVr7T4GbI2IosB64Ilv3kKmhfbpwwQlHce9LK1x1mFney2bFMRJYGhHLImIn8CBwYfoBETEzIrYmm7OAkgNdUJKAsaSSDMDvgPGNGfShunpsGdt3u+ows/yXzcTRD6hM265K2vbnCuDPadvtJVVImiVpfNLWC9gQEbsPdk1Jk5PzK6qrqw/pBhpiaJ/Oe6uOD111mFkeaxaD45IuBcqBm9KaB0REOTAJuEXSkIZcMyKmRkR5RJQXFxc3YrT7d/XYMnbs3sNdfq/DzPJYNhPHKqA0bbskafsYSWcDPwYuiIi9/1SPiFXJz2XAM8AIYC3QXVKbA10zV/ZWHS+76jCz/JXNxDEXKEuegioCJgDT0w+QNAK4k1TSWJPW3kNSu+R7b+B0YHFEBDATuCg59HLg8SzeQ4NdfZarDjPLb1lLHMk4xBTgCeAN4KGIWCTpBkm1T0ndBHQGHq7z2O0ngQpJC0gliv8dEYuTfT8ErpO0lNSYx2+ydQ+HYkhxZy4c3s9Vh5nlLaX+EZ/fysvLo6Kiosl+3zvVWzjnF8/yvz4zmH8875NN9nvNzBqTpHnJWPPHNIvB8XzzUdWx3FWHmeUdJ44suXrsUHburuHOZ9/JdShmZo3KiSNLBhd3Zvzwftw3awXVm111mFn+cOLIoilJ1TH1OVcdZpY/nDiyKL3qWLN5e67DMTNrFE4cWXb1WWWpquNZv9dhZvnBiSPLBvXuxPgR/fjP2a46zCw/OHE0gavHlrFrT7jqMLO84MTRBAb17sT44a46zCw/OHE0kavHDmXXnuBOVx1m1sI5cTSRgbVVh5+wMrMWzomjCV09dii7a4JfPeOqw8xaLieOJjSwdye+OKIf989ewZpNrjrMrGVy4mhiU8YkVYfHOsyshXLiaGKuOsyspXPiyIHasY47PHOumbVAThw5MKBXJ740oh/TZq901WFmLU5WE4ekcZLekrRU0vX17L9O0mJJCyU9LWlA0j5c0suSFiX7Lkk75x5J7yZLzc6XNDyb95AtU1x1mFkLlbXEIakQuB34HDAMmChpWJ3DXgXKI+J44BHgZ0n7VuCyiDgWGAfcIql72nnfj4jhyWd+tu4hmwb06sSXT+zH/bNX8oGrDjNrQbJZcYwElkbEsojYCTwIXJh+QETMjIityeYsoCRpXxIRbyff3wPWAMVZjDUnpowpY09NcMczrjrMrOXIZuLoB1SmbVclbftzBfDnuo2SRgJFQPpf1xuTLqybJbWr72KSJkuqkFRRXV3d8OibQP9eHfnyif2YNsdVh5m1HM1icFzSpUA5cFOd9iOB+4BvRERN0vwj4BjgZKAn8MP6rhkRUyOiPCLKi4ubb7EyZUwZNa46zKwFyWbiWAWUpm2XJG0fI+ls4MfABRGxI629K/BH4McRMau2PSLej5QdwN2kusRarFTVUcK0OStZvdFVh5k1f9lMHHOBMkmDJBUBE4Dp6QdIGgHcSSpprElrLwIeA+6NiEfqnHNk8lPAeOD1LN5Dk7hqzFBqaoJf+QkrM2sBspY4ImI3MAV4AngDeCgiFkm6QdIFyWE3AZ2Bh5NHa2sTy1eA0cDX63ns9n5JrwGvAb2Bn2TrHpqKqw4za0kUEbmOIevKy8ujoqIi12EcUOW6rYz5+TN8dVR//vXC43IdjpkZkuZFRHnd9mYxOG5Q2rMjF51UwgNzKl11mFmz5sTRjFw1Zig1EdzxzNJch2Jmtl9OHM1Iac+OXFyeqjre37gt1+GYmdXLiaOZufLM2qrDT1iZWfPkxNHM1FYdD7rqMLNmyomjGaod6/iPma46zKz5OWjikHStpK5K+Y2kVySd2xTBtVYlPTpycXkpv59byXsbXHWYWfOSScXxzYjYBJwL9AC+BvzvrEZlXDVmiMc6zKxZyiRxKPl5HnBfRCxKa7MscdVhZs1VJoljnqQnSSWOJyR1AWoOco41gqvGDCEI/sPvdZhZM5JJ4rgCuB44OVl0qS3wjaxGZYCrDjNrnjJJHKcCb0XEhmTdjP8H2JjdsKzWVWOGArjqMLNmI5PEcQewVdIJwD+QWonv3qxGZXv1696BryRVxypXHWbWDGSSOHZHagrdC4HbIuJ2oEt2w7J0V9ZWHTNddZhZ7mWSODZL+hGpx3D/KKmA1DiHNZHaquOhClcdZpZ7mSSOS4AdpN7nWE1qCdibDnyKNbarXHWYWTNx0MSRJIv7gW6Szge2R0RGYxySxkl6S9JSSdfXs/86SYslLZT0tKQBafsul/R28rk8rf0kSa8l1/xlsoRs3juqewcuOTlVdVSt35rrcMysFctkypGvAHOAi0kt6Tpb0kUZnFcI3A58DhgGTJQ0rM5hrwLlEXE88Ajws+TcnsC/AKOAkcC/SOqRnHMH8HdAWfIZd7BY8sWVZ9Y+YeW3yc0sdzLpqvoxqXc4Lo+Iy0j9If+nDM4bCSyNiGURsRN4kNQA+14RMTN5NwRgFqluMIDPAk9FxLqIWA88BYyTdCTQNSJmJQP29wLjM4glL9RWHQ+76jCzHMokcRRExJq07bUZntcPqEzbrkra9ucK4M8HObdf8v2g15Q0WVKFpIrq6uoMwm0ZrjxzKELc7plzzSxHMkkAf5H0hKSvS/o68EfgT40ZRPJiYTmNOOgeEVMjojwiyouLixvrsjnnqsPMci2TwfHvA1OB45PP1Ij4YQbXXgWUpm2XJG0fI+lsUt1hF0TEjoOcu4qPurP2e818d+WYIRTIVYeZ5UZGCzlFxKMRcV3yeSzDa88FyiQNklQETACmpx8gaQRwJ6mkkd4d9gRwrqQeyaD4ucATEfE+sEnSKcnTVJcBj2cYT944slsHJoxMVR2V61x1mFnT2m/ikLRZ0qZ6PpslbTrYhSNiNzCFVBJ4A3goIhZJukHSBclhNwGdgYclzZc0PTl3HfBvpJLPXOCGpA3gSuDXwFJS05/Ujou0Kt8+M1V1eA4rM2tqSj2clN/Ky8ujoqIi12E0un9+/HWmzV7JzO+dSWnPjrkOx8zyjKR5EVFet91rjrdgV5451FWHmTU5J44W7Ihu7Zk4spSHK6o81mFmTcaJo4X7dlJ13O45rMysiThxtHC1Vccj81x1mFnTyGSuqi8lEw1ubMhTVdZ0vn3mUAoKxG0zXHWYWfZlUnH8jNR7Ft0iomtEdImIrtkOzDJ3RLf2TBrZn0dfqWLlWlcdZpZdmSSODyLijaxHYofl22cOoaDAYx1mln2ZJI4KSb+XNDHptvqSpC9lPTJrkL5dU1XHI646zCzLMkkcXYGtpKb9+ELyOT+bQdmh+faZQygsELfNfDvXoZhZHmtzsAMi4htNEYgdvtqq475ZK5gypoz+vfw2uZk1vgPNVfWD5OetyRKtH/s0XYjWEFeeOYQ2rjrMLIsOVHHUDojn3yRPeaxP1/ZMGtWfe19ewVVjhjKgV6dch2RmeWa/iSMi/pD8/F3ThWON4dtnDGHa7JXcNmMpN118Qq7DMbM8k8kLgMWSfi7pT5Jm1H6aIjg7NH26tuerowbwX6+uYsXav+U6HDPLM5k8VXU/qW6rQcC/AstJrZFhzdjfnzGYNgXiVr9NbmaNLJPE0SsifgPsiohnI+KbwNgsx2WHqbbqeOzVVSz/0FWHmTWeTBLHruTn+5I+nyz32jOLMVkjqa06bvPb5GbWiDJJHD+R1A34B+B7pJZt/W4mF5c0TtJbkpZKur6e/aMlvSJpt6SL0trHJEvJ1n62Sxqf7LtH0rtp+4ZnEktr1Kdrey49xVWHmTWuAyYOSYVAWURsjIjXI2JMRJwUEdMPduHk3NuBzwHDgImShtU5bCXwdWBaemNEzIyI4RExnFS32FbgybRDvl+7PyLmHyyW1uxbHusws0Z2wMQREXuAiYd47ZHA0ohYFhE7gQeBC+tcf3lELARqDnCdi4A/R4QnYDoEfbqkqo7/nu+qw8waRyZdVS9Kuk3SZySdWPvJ4Lx+QGXadlXS1lATgAfqtN0oaaGkmyW1q+8kSZMlVUiqqK6uPoRfmz++dcZg2ha66jCzxpFJ4hgOHAvcAPyf5PPzLMa0l6QjgU8BT6Q1/wg4BjiZ1CD9D+s7NyKmRkR5RJQXFxdnPdbmrE+X9lw6agCPvVrFu646zOwwZZI4rkjGNvZ+gP+VwXmrgNK07ZKkrSG+AjwWEbVPdhER70fKDuBuUl1idhDfOmMIRW0KuHWG57Ays8OTSeJ4pJ62hzM4by5QJmmQpCJSXU4HHVSvYyJ1uqmSKgRJAsYDrzfwmq1ScZd2fO2UAfz3q6tcdZjZYTnQ7LjHSPoy0C19ASdJXwfaH+zCEbEbmEKqm+kN4KGIWCTpBkkXJL/jZElVwMXAnZIWpf3+gaQqlmfrXPp+Sa8BrwG9gZ9kfrut2+TRSdXxtKsOMzt0B5od9xOkFmzqTmrxplqbgb/L5OIR8SfgT3Xa/jnt+1xSXVj1nbucegbTI8JvrR+i2qrjNy+8y5SxQxlc3DnXIZlZC3Sg2XEfBx6XdGpEvNyEMVkWTR49hPtmreC2GUv5xSXDcx2OmbVABx3jcNLIL8Vd2nHZqQP57/mrWFa9JdfhmFkLlMnguOWZyaMHJ09Y+b0OM2s4J45WqHfnVNXx+PxVvOOqw8waKJOFnNpJmiTpHyX9c+2nKYKz7Jk8ejDt2hRym6sOM2ugTCqOx0nNMbUb+Fvax1qwVNUxwFWHmTXYgR7HrVUSEeOyHok1ub8bPZh7X17BrU+/zS0TRuQ6HDNrITKpOF6S9KmsR2JNrnfndlx22gCmL3iPpWtcdZhZZg705vhrkhYCnwZeSRZkWpjWbnlg8mdqxzr8NrmZZeZAXVXnN1kUljO9kqrjrueWMWVsGUP7+G1yMzuw/VYcEbEiIlYARwLr0rbXA0c0VYCWfZM/M5j2bQs9c66ZZSSTMY47gPQO8C1Jm+WJXsl7HR7rMLNMZJI4FBFRuxERNWT2NJa1IH/3mUF0aFvILz1zrpkdRCaJY5mkayS1TT7XAsuyHZg1rdqq4w8L32Ppms25DsfMmrFMEsffA6eRWr1vFTAKmJzNoCw3Jo8eTIe2hUy+dx5/eu19amri4CeZWauTyey4ayJiQkT0ST6TImJNUwRnTatnpyLuuPQkJLjy/lf4/K0v8OSi1aT1VJqZZTRXVYmkxyStST6PSqp38SVr+c44upgnv3sGN19yAtt27mbyffO44LYXmfnmGicQMwMy66q6m9Ra4Uclnz8kbQclaVzy4uBSSdfXs3+0pFck7ZZ0UZ19eyTNTz7T09oHSZqdXPP3yXrm1ogKC8QXR5Tw1+vO4GcXHc/6rTv5xj1z+dIdL/H829VOIGatnA72R0DS/IgYfrC2es4rBJYA5wBVwFxgYkQsTjtmINAV+B4wPSIeSdu3JSL2eRtN0kPAf0XEg5J+BSyIiAM+HlxeXh4VFRUHvE/bv527a3hkXhW3zXib9zZuZ+TAnlx37tGcMrhXrkMzsyySNC8iyuu2Z1JxrJV0qaTC5HMpsDaD80YCSyNiWUTsBB4kNcvuXhGxPCIWAjUZXA9JAsYCtQnmd8D4TM61Q1fUpoBJo/oz8/tncsOFx7J87d+YMHUWX/31LOatWJfr8MysiWWSOL4JfAVYnXwuAr6RwXn9gMq07aqkLVPtJVVImiVpfNLWC9gQEbsPdk1Jk5PzK6qrqxvwa21/2rUp5LJTB/LcD8bwT+cP463Vm/nyHS9z+W/nML9yQ67DM7MmctAX+ZJpRi5ogljqGhARqyQNBmZIeg3YmOnJETEVmAqprqosxdgqtW9byBWfHsTEkaXc9/IKfvXsO4y//UXO/mQfvnP20RzXr1uuQzSzLMrkqarBkv4gqTp5qurx5I/5wawCStO2S5K2jETEquTnMuAZYASpLrLukmoTXoOuaY2rY1EbvnXGEJ7/4Vi+d+7RzHl3Heff+gJ/f9883ly9KdfhmVmWZNJVNQ14iNRkh0cBDwMPZHDeXKAseQqqCJhA6umsg5LUQ1K75Htv4HRgcTL1yUxS3WUAl5NaodByqHO7NkwZW8YL14/l2rPKeHHph3zu/z7PlGmveO4rszyUyVNVCyPi+DptCyLihINeXDoPuAUoBH4bETdKugGoiIjpkk4GHgN6ANuB1RFxrKTTgDtJDZoXALdExG+Saw4mNdDeE3gVuDQidhwoDj9V1bQ2bN3JXc8v4+4Xl7N91x4uHN6Pa88qY2DvTrkOzcwaYH9PVWWSOH5Kair1B4EALiH1h/4mgIho9o/VOHHkxtotO5j63DJ+9/Jydu0JvjSiH9ecVUZpz465Ds3MMnA4iePdA+yOiMhkvCOnnDhya83m7fzqmWX85+wV1NQEF5eXcvXYoRzVvUOuQzOzAzjkxJEPnDiah9Ubt/MfzyzlgTkrEWLiyFKuHDOUvl3b5zo0M6tHg18AlPSDtO8X19n3740bnrUGR3Rrzw0XHscz3x/Dl08q4f7ZKxn9s5n82/8spnrzAYepzKwZ2W/FIemViDix7vf6tps7VxzN08q1W7l1xtv816urKCos4LLTBvCt0UPo2cnTj5k1B4cy5Yj2872+bbMG69+rIzddfAJ/ve4MPntsX6Y+t4zP/HQGP3/iLTZu3ZXr8MxsPw6UOGI/3+vbNjtkg3p34pYJI3jyO6M585g+3DZzKZ/+6Qxu+esSNm13AjFrbg7UVbUH+Bup6qIDsLV2F9A+Ito2SYSNwF1VLcsb72/ilr8u4YlFH9CtQ1smjx7M108bSKd2XurerCn5qSonjhbn9VUbufmpJTz95hp6diriW6MHc9mpA+lQVJjr0MxaBScOJ44W69WV67n5r2/z3JJqendux5VnDmHSqP60b+sEYpZNThxOHC3e3OXruPmpJbz0zlr6dm3HlDFD+crJpbRr4wRilg1OHE4ceePld9byi6feYu7y9fTr3oEpY4dy0UkltC3MZM5OM8uUE4cTR16JCF5Y+iH/58klzK/cQGnPDlwztowvjuhHGycQs0bhxOHEkZcigmfequYXTy3htVUbGdS7E9eeVcYXTjiKwgK/bmR2OA5nzXGzZksSY47pw/Qpp3Pn106iXZsCvvP7+Yy75Tn+uPB9amry/x9GZk3NicPygiQ+e+wR/Omaz3D7pBMJ4Kppr3DeL5/niUWraQ2VtVlTceKwvFJQID5//JE88Z3R/N8Jw9mxu4Zv3TePL9z2AjPe/MAJxKwRZDVxSBon6S1JSyVdX8/+0ZJekbRb0kVp7cMlvSxpkaSFki5J23ePpHclzU8+w7N5D9YyFRaIC4f346nvjubnF5/Apm27+eY9FXzxP17iuSXVTiBmhyFrg+OSCoElwDlAFak1yCdGxOK0YwYCXYHvAdMj4pGk/WhSi0S9LekoYB7wyYjYIOke4H9qj82EB8dt154aHp1Xxa0zlrJqwzZOHtiD755zNKcN6Z3r0MyarVwMjo8ElkbEsojYSWrp2QvTD4iI5RGxkNTa4untSyLi7eT7e8AaoDiLsVqea1tYwISR/ZnxvTP4t/HHUbluG5Pums3EqbOoWN7sVz82a1aymTj6AZVp21VJW4NIGgkUAe+kNd+YdGHdLKndfs6bLKlCUkV1dXVDf63lqXZtCvnaKQN45vtn8i9fGMbba7Zw0a9e5st3vMSj86rYvmtPrkM0a/aa9eC4pCOB+4BvRERtVfIj4BjgZKAn8MP6zo2IqRFRHhHlxcUuVuzj2rct5BunD+L5H4zhn84fxvq/7eQfHl7AyBv/yv87fRFvf7A51yGaNVvZnKd6FVCatl2StGVEUlfgj8CPI2JWbXtEvJ983SHpblLjI2aHpENRIVd8ehDfPH0gs5atY9qcldw/ewX3vLSckwf2YOLI/pz3qSM9oaJZmmwmjrlAmaRBpBLGBGBSJidKKgIeA+6tOwgu6ciIeF+SgPHA640atbVKkjh1SC9OHdKLtVuG8egrVTwwp5LrHlrAv/5hMV86sR+TRvanrG+XXIdqlnNZnXJE0nnALUAh8NuIuFHSDUBFREyXdDKpBNED2A6sjohjJV0K3A0sSrvc1yNivqQZpAbKBcwH/j4ithwoDj9VZYciInh52VqmzV7JE4tWs2tPcPLAHkwa1Z/PHecqxPKf56py4rDD8OGWHTw6r4oH5qxk+dqtdO/Yli+NKGHSqFKG9nEVYvnJicOJwxpBTU0wa9la7p+zkieTKmTkoJ5MGtmfcccd4SrE8ooThxOHNbIPt+zgkaQKWZFUIV8+sYSJI/sztE/nXIdndticOJw4LEtqaj4+FrK7JlWFfHVUfz57rKsQa7n2lziy+VSVWatQUCBOH9qb04f2pnrzR1XItQ/Op0dtFTKqP0OKXYVYfnDFYZYFNTXBS++sZdqcFTy56AN21wSjBvVk0qjUWIjXSbeWwF1VThyWI9Wbd/DwvEoenFPJynVb6dGxLRedVMKEka5CrHlz4nDisByrqQlefOdDps1eyVOLU1XIKYN7MmnUAD57bF9XIdbsOHE4cVgzsmbzdh6uqOLBuSupXLeNnp2KuOik1BNZg3p3ynV4ZoAThxOHNUs1NcELSz/kgTkfVSGnDu7FpFH9OddViOWYE4cThzVzazZt5+Hkiayq9akq5OJkLMRViOWCE4cTh7UQNTXB80s/ZNrsFfz1jTXsqQlOG9KLiSNT74UUtWnWqyFYHnHicOKwFmjNpu08VFHJA3MqWbVhG706FXFReQkTT+7PQFchlmVOHE4c1oLtqQmef7uaB+as3FuFnD40VYWcO8xViGWHE4cTh+WJDzZt56G5lTw411WIZZcThxOH5Zk9NcFzb1fzwOyVPP1mqgr59NDeTBzZn3OG9XUVYofNicOJw/LY6o2psZDfJ1VI785FXFxeyoSTSxnQy1WIHRonDicOawX21ATPLalm2pyVPP3GB9QEfKbsoyqkbaGrEMvc/hJHVv9fJGmcpLckLZV0fT37R0t6RdJuSRfV2Xe5pLeTz+Vp7SdJei255i+TtcfNDCgsEGOO6cNdl5Xz0vVn8d2zj+adNVu48v5XOPX/m8FP//ImK9duzXWY1sJlreKQVAgsAc4BqoC5wMSIWJx2zECgK/A9YHpEPJK09wQqgHIggHnASRGxXtIc4BpgNvAn4JcR8ecDxeKKw1qzPTXBs0vWMG12JTPe/KgKOePoYoaXdufYo7rRochvqNu+crEex0hgaUQsSwJ4ELgQ2Js4ImJ5sq+mzrmfBZ6KiHXJ/qeAcZKeAbpGxKyk/V5gPHDAxGHWmhUWiLHH9GXsMX15f+M2HppbxaOvVPGTP76xd/8n+nbhhNLuDC/txgml3Snr04XCAhfzVr9sJo5+QGXadhUw6jDO7Zd8qupp34ekycBkgP79+2f4a83y25HdOnDt2WVce3YZazZvZ2HlRhZUbWB+5Qb+uPA9HpizEoCORYUcd1Q3TkgSyQkl3Snp0QH3DBvk8QqAETEVmAqprqoch2PW7PTp0p6zh7Xn7GF9AYgIlq/dyoLKVCJZULWB3728gp3PvwtAr05Fe5PICaXdOKGkOz06FeXyFixHspk4VgGladslSVum555Z59xnkvaSQ7ymmR2AJAb17sSg3p0YPyJVyO/cXcOSDzanEkmSTGa+tYbaodEBvTpyQkl3ji/p5vGSViSbiWMuUCZpEKk/7hOASRme+wTw75J6JNvnAj+KiHWSNkk6hdTg+GXArY0ct5klitoUcFy/bhzXrxuXnjIAgC07dvNaVaqLa0HlBiqWr2P6gvcAj5e0Fll9j0PSecAtQCHw24i4UdINQEVETJd0MvAY0APYDqyOiGOTc78J/GNyqRsj4u6kvRy4B+hAalD86jjITfipKrPsWrNpOwuqNu6tShZUbmDT9t1AMl7SL1WR1HZz9evu8ZKWwC8AOnGYNZmammDFuo+Plyx6bxM7d6ceoPR4ScuQi8dxzayVKiiof7zkrdWbmZ9UJAsq6x8vqe3mOvaobrRv6/GS5sgVh5nlzObtu3ht1UYWVKa6uRZWbeC9jduB1HjJMUd04fgSj5fkiruqnDjMWoS64yXzKzew2eMlOeGuKjNrEfp0bc85w9pzTvJ+SU1NsHzt35JB943Mr9zAPS8uZ+ee1HhJ785Fe7u4UuMm3eje0eMl2eTEYWbNWkGBGFzcmcHFnfniiNRrXDt31/Dm6k1JVZKqTmZ4vKTJOHGYWYtT1KaA40u6c3xJd76WtNUdL5mb9n5JmwJR1rcLg3t3orRnR0p7dqB/z46U9ujIUd07eNGrBnLiMLO80KV9W04b0pvThvTe2/bBpu17x0peW7WJxe9v4snFq9m156Ox3QKl5vAq6ZEkkzqJpbhLO4+h1OHEYWZ5q2/X9px77BGce+wRe9v21AQfbNrOynVbqVy3lcr121I/123l2SXVrNm842PXaN+2gJIeHZNE0iFJLKmkUtqzA13at23q28o5Jw4za1UKC8RR3TtwVPcOnDK41z77t+/aQ9X6rVSu20bl+q2sXLs19XPdNua+u47NO3Z/7PgeHdvSv2dHSpJk0j+tYjmqe4e8XHXRicPMLE37toUM7dOFoX267LMvIti4bVdSrWxL/VyfqlYWrdrIk4vq7wYr7dkhLal81B1W3LlldoM5cZiZZUgS3TsW0b1jEceXdN9n/56aYPWm7VSu28rKdVupSrrCVh6gGyzV5ZVKKiVJV1htguncrnn+iW6eUZmZtUCFBaJf9w70O0g3WG3FUptgKtdvY86769hSpxusZ6ciSnt0oKQ2maR1heWyG8yJw8ysiRysG2zD1l3JeMpHXWFV61PdYE+8vprdNfV3g+1NKr06UtIj+91gThxmZs2AJHp0KqJHpwN3g9UO1lemPRU2861qqut0g3VoW0hJjw7c+bWTGFzcuVFjdeIwM2sB0rvBTmXfbrBtO5OnwdanDdyv25qV6VecOMzM8kCHokLK+nahrO++3WCNLasjK5LGSXpL0lJJ19ezv52k3yf7Z0samLR/VdL8tE+NpOHJvmeSa9bu65PNezAzs4/LWuKQVAjcDnwOGAZMlDSszmFXAOsjYihwM/BTgIi4PyKGR8Rw4GvAuxExP+28r9buj4g12boHMzPbVzYrjpHA0ohYFhE7gQeBC+sccyHwu+T7I8BZ2vcxgInJuWZm1gxkM3H0AyrTtquStnqPiYjdwEbYZ9TnEuCBOm13J91U/1RPojEzsyxq1pOoSBoFbI2I19OavxoRnwI+k3y+tp9zJ0uqkFRRXV3dBNGambUO2Uwcq4DStO2SpK3eYyS1AboBa9P2T6BOtRERq5Kfm4FppLrE9hERUyOiPCLKi4uLD+M2zMwsXTYTx1ygTNIgSUWkksD0OsdMBy5Pvl8EzIhkEXRJBcBXSBvfkNRGUu/ke1vgfOB1zMysyWTtPY6I2C1pCvAEUAj8NiIWSboBqIiI6cBvgPskLQXWkUoutUYDlRGxLK2tHfBEkjQKgb8Cd2XrHszMbF+KiIMf1cJJqgZWHOLpvYEPGzGclsD33Dr4nvPf4d7vgIjYp6+/VSSOwyGpIiLKcx1HU/I9tw6+5/yXrftt1k9VmZlZ8+PEYWZmDeLEcXBTcx1ADvieWwffc/7Lyv16jMPMzBrEFYeZmTWIE4eZmTWIE8cBHGw9kXwj6beS1khqFW/jSyqVNFPSYkmLJF2b65iyTVJ7SXMkLUju+V9zHVNTkVQo6VVJ/5PrWJqCpOWSXksmhK1o1Gt7jKN+yXoiS4BzSM3sOxeYGBGLcxpYFkkaDWwB7o2I43IdT7ZJOhI4MiJekdQFmAeMz/P/jQV0iogtyQwMLwDXRsSsHIeWdZKuA8qBrhFxfq7jyTZJy4HyiGj0Fx5dcexfJuuJ5JWIeI7U1C+tQkS8HxGvJN83A2+w79T/eSVStiSbbZNP3v/rUVIJ8Hng17mOJR84cexfJuuJWJ5Ili0eAczOcShZl3TZzAfWAE9FRN7fM3AL8AOgJsdxNKUAnpQ0T9LkxrywE4e1epI6A48C34mITbmOJ9siYk+yLHMJMFJSXndLSjofWBMR83IdSxP7dEScSGr57quSruhG4cSxf5msJ2ItXNLP/yhwf0T8V67jaUoRsQGYCYzLcSjZdjpwQdLn/yAwVtJ/5jak7Etbu2gN8Bj7WbvoUDhx7F8m64lYC5YMFP8GeCMifpHreJqCpGJJ3ZPvHUg9/PFmToPKsoj4UUSURMRAUv8dz4iIS3McVlZJ6pQ88IGkTsC5NOLaRU4c+5GsgV67nsgbwEMRsSi3UWWXpAeAl4FPSKqSdEWuY8qy00ktPTw2eWRxvqTzch1Ulh0JzJS0kNQ/jp6KiFbxeGor0xd4QdICYA7wx4j4S2Nd3I/jmplZg7jiMDOzBnHiMDOzBnHiMDOzBnHiMDOzBnHiMDOzBnHiMGsEkvakPdI7vzFnU5Y0sLXMWGwtQ5tcB2CWJ7Yl03iY5T1XHGZZlKyJ8LNkXYQ5koYm7QMlzZC0UNLTkvon7X0lPZasl7FA0mnJpQol3ZWsofFk8ta3WU44cZg1jg51uqouSdu3MSI+BdxGapZWgFuB30XE8cD9wC+T9l8Cz0bECcCJQO1sBWXA7RFxLLAB+HJW78bsAPzmuFkjkLQlIjrX074cGBsRy5IJFVdHRC9JH5JaRGpX0v5+RPSWVA2URMSOtGsMJDU1SFmy/UOgbUT8pAluzWwfrjjMsi/2870hdqR934PHJy2HnDjMsu+StJ8vJ99fIjVTK8BXgeeT708D34a9Cy51a6ogzTLlf7WYNY4Oyap6tf4SEbWP5PZIZqPdAUxM2q4G7pb0faAa+EbSfi0wNZmZeA+pJPJ+toM3awiPcZhlUTLGUR4RH+Y6FrPG4q4qMzNrEFccZmbWIK44zMysQZw4zMysQZw4zMysQZw4zMysQZw4zMysQf5/ibPgiNNObKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate training\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_loss_tensor_train.numpy())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch train loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tender-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9781\n",
      "Training set cost: 0.0672\n",
      "Validation set accuracy: 0.9661\n",
      "Validation set cost: 0.1154\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model performance\n",
    "\n",
    "loss_tensor_train = torch.zeros(len(train_loader))\n",
    "correct_train = 0\n",
    "total_train = len(normalized_mnist)\n",
    "\n",
    "loss_tensor_val = torch.zeros(len(val_loader))\n",
    "correct_val = 0\n",
    "total_val = len(normalized_mnist_val)\n",
    "\n",
    "with torch.no_grad(): # Forward propagation does not accumulate gradients\n",
    "    \n",
    "    for i, batch_train in enumerate(train_loader):\n",
    "        imgs_train, labels_train = batch_train\n",
    "        \n",
    "        outputs_train = model(imgs_train.view(batch_size, -1))\n",
    "        loss_train = loss_fn(outputs_train, labels_train)\n",
    "        loss_tensor_train[i] = loss_train.detach()\n",
    "        _, predicted_labels_train = torch.max(outputs_train, dim=1)\n",
    "        correct_train += int((predicted_labels_train == labels_train).sum())\n",
    "    \n",
    "    for i, batch_val in enumerate(val_loader):\n",
    "        imgs_val, labels_val = batch_val\n",
    "        \n",
    "        outputs_val = model(imgs_val.view(batch_size, -1))\n",
    "        loss_val = loss_fn(outputs_val, labels_val)\n",
    "        loss_tensor_val[i] = loss_val.detach()\n",
    "        _, predicted_labels_val = torch.max(outputs_val, dim=1)\n",
    "        correct_val += int((predicted_labels_val == labels_val).sum())\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"Training set accuracy: {:0.4f}\".format(correct_train/total_train),\n",
    "      \"Training set cost: {:0.4f}\".format(loss_tensor_train.mean(dim=0)),\n",
    "      \"Validation set accuracy: {:0.4f}\".format(correct_val/total_val),\n",
    "      \"Validation set cost: {:0.4f}\".format(loss_tensor_val.mean(dim=0)),\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
