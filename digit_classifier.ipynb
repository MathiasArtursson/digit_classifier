{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coral-settle",
   "metadata": {},
   "source": [
    "# Digit classifier with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-radical",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-zealand",
   "metadata": {},
   "source": [
    "*%matplotlib inline* allows for plots inside the notebook.\n",
    "Edge items for the print options for tensors are limited, which prints less information for tensors.\n",
    "A manual seed is also set for pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confidential-shame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f432de89930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-bracelet",
   "metadata": {},
   "source": [
    "Training device is set, which is either the cpu (slower) or gpu (faster). *torch.cuda.is_available()* evaluates if a compatible gpu is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "revised-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathias/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(\"Training device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-geography",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-wilson",
   "metadata": {},
   "source": [
    "Datasets are downloaded into a new folder in the current directory, if not already found there. Samples are stored as tuples, where images are tensors and labels are integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "verbal-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "data_path = data_path = os.getcwd() + '/dataset'\n",
    "\n",
    "tensor_mnist = datasets.MNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "tensor_mnist_val = datasets.MNIST(data_path, train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-peace",
   "metadata": {},
   "source": [
    "Information for the datasets and samples are presented. Note that the axes are needed to be changed from C × H × W to H × W × C to allow for plots. This is done with *permute*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "final-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 60000\n",
      "Number of samples in validation set: 10000\n",
      "Sample shape: torch.Size([1, 28, 28])\n",
      "Pixel data type: torch.float32\n",
      "Minimum pixel value: 0.0\n",
      "Maximum pixel value: 1.0\n",
      "\n",
      "Example with label = 5:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_t, label = tensor_mnist[0]\n",
    "\n",
    "print(\"Number of samples in training set: {}\".format(len(tensor_mnist)),\n",
    "      \"Number of samples in validation set: {}\".format(len(tensor_mnist_val)),\n",
    "      \"Sample shape: {}\".format(img_t.shape),\n",
    "      \"Pixel data type: {}\".format(img_t.dtype),\n",
    "      \"Minimum pixel value: {}\".format(img_t.min()),\n",
    "      \"Maximum pixel value: {}\".format(img_t.max()),\n",
    "      sep = '\\n')\n",
    "\n",
    "print(\"\\nExample with label = {}:\".format(label))\n",
    "# Axes are changed from C × H × W to H × W × C to allow for plots.\n",
    "plt.imshow(img_t.permute(1, 2, 0)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-permission",
   "metadata": {},
   "source": [
    "Data is normalized to ease training further on. This is done as follows:\n",
    "$$\\tilde{x}^i_j = \\frac{x^i_j - \\mu_j}{\\sigma_j}$$\n",
    ", where $\\tilde{x}^i_j$ is the normalized data from sample $i$ and feature $j$, $x^i_j$ the raw data from sample $i$ and feature $j$, $\\mu_j$ the mean over all samples for feature $j$, and $\\sigma_j$ the standard deviation over all sampltes for feature $j$.\n",
    "\n",
    "The mean and standard deviation for all samples are computed by stacking them.\n",
    "\n",
    "The stacked tensor is unrolled using *view*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "reasonable-interstate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked data: torch.Size([1, 28, 28, 60000])\n",
      "Mean of all pixels: 0.1307\n",
      "Standard deviation of all pixels: 0.3081\n"
     ]
    }
   ],
   "source": [
    "# Stack samples\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_mnist], dim=3)\n",
    "print(\"Stacked data: {}\".format(imgs.shape))\n",
    "\n",
    "\n",
    "# view(1, -1) keeps the first channel and merges all the remaining dimensions into one. \n",
    "# Our 1 × 28 × 28 x 60000 images are transformed into a 1 x (28*28*60000) tensor.\n",
    "imgs_mean = imgs.view(1, -1).mean(dim=1)\n",
    "imgs_std = imgs.view(1, -1).std(dim=1)\n",
    "\n",
    "print(\"Mean of all pixels: {:0.4f}\".format(imgs_mean[0]),\n",
    "      \"Standard deviation of all pixels: {:0.4f}\".format(imgs_std[0]),\n",
    "      sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-diversity",
   "metadata": {},
   "source": [
    "Normalized datasets are fetched using the computed mean and standard deviation. \n",
    "\n",
    "Note that the validation set is normalized with the training set mean and standard deviation. This is because the model will be shaped by the training set. The validation set must therefore have the same data preproccessing, so that the model will recognize validation set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "green-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mnist = datasets.MNIST(data_path, train=True, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(imgs_mean, imgs_std)]))\n",
    "\n",
    "normalized_mnist_val = datasets.MNIST(data_path, train=False, download=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(imgs_mean, imgs_std)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-solomon",
   "metadata": {},
   "source": [
    "The datasets are prepared for training and evaluation by being put in data loaders. This makes the data compatible with forward- and backpropagation methods. \n",
    "\n",
    "The data is also divided into batches. This is made because the training set is very large, and using the whole dataset for forward- and backpropagation would be very computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "skilled-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of training images: torch.Size([16, 1, 28, 28])\n",
      "Batch of training labels: torch.Size([16])\n",
      "Batch of validation images: torch.Size([16, 1, 28, 28])\n",
      "Batch of validation labels: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(normalized_mnist, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(normalized_mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for imgs_train, labels_train in train_loader:\n",
    "    print(\"Batch of training images: {}\".format(imgs_train.shape),\n",
    "          \"Batch of training labels: {}\".format(labels_train.shape),\n",
    "          sep = \"\\n\")\n",
    "    break\n",
    "for imgs_val, labels_val in val_loader:\n",
    "    print(\"Batch of validation images: {}\".format(imgs_val.shape),\n",
    "          \"Batch of validation labels: {}\".format(labels_val.shape),\n",
    "          sep = \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-photography",
   "metadata": {},
   "source": [
    "Large obsolete objects are deleted from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "imperial-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imgs\n",
    "del tensor_mnist\n",
    "del tensor_mnist_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-vienna",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-tennis",
   "metadata": {},
   "source": [
    "A neural network is initialized with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "finite-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize linear neural network model with one hidden layer. \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "hidden_features = 250\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, hidden_features),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(hidden_features, 10),\n",
    "    nn.LogSoftmax(dim=1))          # Converts output vector first to proabilities by\n",
    "                                   # applying function torch.exp(x) / torch.exp(x).sum()\n",
    "                                   # It then takes the log of this. We need the natural logarithmic\n",
    "                                   # expression for the cost later on, and doing the \n",
    "                                   # calculation of ln in the model helps stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "humanitarian-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw image shape: torch.Size([1, 28, 28]), Raw image stride: (1, 28, 1)\n",
      "Unrolled image shape: torch.Size([1, 784]), Unrolled image stride: (784, 1)\n",
      "Label tensor shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Shaping input data\n",
    "# This is done by unrolling the 28x28 image into a 1D vector.\n",
    "# Unrolling is done through \"view\", which changes the stride \n",
    "# of the contiguous linear container of numbers in memory.\n",
    "# view(-1) changes the stride so that the resulting shape becomes\n",
    "# torch.Size([a]), where \"a\" is \"however many indexes are\n",
    "# left, given the other dimensions and the original number of elements.”\n",
    "\n",
    "img, label = normalized_mnist[0]\n",
    "img_unrolled = img.view(-1).unsqueeze(0)    # Cost function requires the first dimension to be the batch, hence we need to add one dimension with unsqueeze \n",
    "label_tensor = torch.tensor([label])\n",
    "\n",
    "print(\"Raw image shape: {}, Raw image stride: {}\".format(img.shape, img.stride()),\n",
    "     \"Unrolled image shape: {}, Unrolled image stride: {}\".format(img_unrolled.shape, img_unrolled.stride()),\n",
    "     \"Label tensor shape: {}\".format(label_tensor.shape),\n",
    "     sep = \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "herbal-taste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Output from model: tensor([[-2.4796, -2.4591, -2.5340, -2.2044, -2.1045, -2.2420, -2.5525, -2.0934,\n",
      "         -2.2197, -2.2719]], grad_fn=<LogSoftmaxBackward>)\n",
      "Hypothesis probabilities from model: tensor([[0.0838, 0.0855, 0.0793, 0.1103, 0.1219, 0.1062, 0.0779, 0.1233, 0.1086,\n",
      "         0.1031]])\n",
      "The largest probability from the model is 0.1233, i.e. the predicted label is 7.\n"
     ]
    }
   ],
   "source": [
    "# Invoke model\n",
    "import math\n",
    "\n",
    "out = model(img_unrolled)    # natural logarithm of hypothesis probabilities from model \n",
    "out_probabilities = math.e ** out.detach()\n",
    "largest_probability, predicted_label = torch.max(out_probabilities, dim=1)\n",
    "print(\"Label: {}\".format(label),\n",
    "      \"Output from model: {}\".format(out),\n",
    "      \"Hypothesis probabilities from model: {}\".format(out_probabilities),\n",
    "      \"The largest probability from the model is {:0.4f}, i.e. the predicted label is {}.\".format(largest_probability[0], predicted_label[0]),\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "adjusted-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate loss function\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "amazing-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2420, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate loss for test image\n",
    "loss_fn(out, label_tensor) #Calculates sum(-out[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "convinced-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "generic-affairs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Epoch time: 56.4 s, Train loss: 0.3012\n",
      "Epoch: 1, Epoch time: 80.5 s, Train loss: 0.1375\n",
      "Epoch: 2, Epoch time: 52.1 s, Train loss: 0.0947\n",
      "Epoch: 3, Epoch time: 51.0 s, Train loss: 0.0700\n",
      "Epoch: 4, Epoch time: 51.9 s, Train loss: 0.0540\n",
      "Epoch: 5, Epoch time: 48.9 s, Train loss: 0.0430\n",
      "Epoch: 6, Epoch time: 49.2 s, Train loss: 0.0344\n",
      "Epoch: 7, Epoch time: 49.4 s, Train loss: 0.0275\n",
      "Epoch: 8, Epoch time: 59.5 s, Train loss: 0.0225\n",
      "Epoch: 9, Epoch time: 61.1 s, Train loss: 0.0183\n",
      "Epoch: 10, Epoch time: 52.4 s, Train loss: 0.0151\n",
      "Epoch: 11, Epoch time: 50.6 s, Train loss: 0.0127\n",
      "Epoch: 12, Epoch time: 51.7 s, Train loss: 0.0108\n",
      "Epoch: 13, Epoch time: 55.1 s, Train loss: 0.0092\n",
      "Epoch: 14, Epoch time: 110.5 s, Train loss: 0.0080\n",
      "Epoch: 15, Epoch time: 82.3 s, Train loss: 0.0070\n",
      "Epoch: 16, Epoch time: 100.3 s, Train loss: 0.0062\n",
      "Epoch: 17, Epoch time: 105.0 s, Train loss: 0.0055\n",
      "Epoch: 18, Epoch time: 64.0 s, Train loss: 0.0050\n",
      "Epoch: 19, Epoch time: 68.5 s, Train loss: 0.0046\n",
      "Epoch: 20, Epoch time: 73.5 s, Train loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import timeit\n",
    "\n",
    "n_epochs = 21\n",
    "\n",
    "epoch_loss_tensor_train = torch.zeros(n_epochs)\n",
    "batch_loss_tensor_train = torch.zeros(len(train_loader))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    tic = timeit.default_timer() #Take start time measurement\n",
    "    \n",
    "    for i, batch_train in enumerate(train_loader):\n",
    "        \n",
    "        imgs_train, labels_train = batch_train\n",
    "        imgs_train = imgs_train.to(device=device) #Move tensor to device \n",
    "        labels_train = labels_train.to(device=device) #Move tensor to device\n",
    "        \n",
    "        outputs_train = model(imgs_train.view(batch_size, -1)) #Unrolls images to a tensor with size [batch_size, 784]\n",
    "        \n",
    "        loss_train = loss_fn(outputs_train, labels_train)\n",
    "        batch_loss_tensor_train[i] = loss_train.detach()             # detach removes gradient tracking\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss_tensor_train[epoch] = batch_loss_tensor_train.mean(dim=0)\n",
    "    \n",
    "    toc = timeit.default_timer() #Take stop time measurement\n",
    "    \n",
    "    print(\"Epoch: {}, Epoch time: {:0.1f} s, Train loss: {:0.4f}\".format(epoch, toc-tic, float(epoch_loss_tensor_train[epoch])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "capital-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate training\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_loss_tensor_train = torch.tensor([0.3012, 0.1375, 0.0947,0.0700,0.0540,0.0430,0.0344,0.0275,0.0225,0.0183,0.0151,0.0127,0.0108,0.0092,0.0080,0.0070,0.0062,0.0055,0.0050,0.0046,0.0041])\n",
    "\n",
    "plt.plot(epoch_loss_tensor_train.numpy())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch train loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "tender-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9999\n",
      "Training set cost: 0.0038\n",
      "Validation set accuracy: 0.9818\n",
      "Validation set cost: 0.0571\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model performance\n",
    "\n",
    "loss_tensor_train = torch.zeros(len(train_loader))\n",
    "correct_train = 0\n",
    "total_train = len(normalized_mnist)\n",
    "\n",
    "loss_tensor_val = torch.zeros(len(val_loader))\n",
    "correct_val = 0\n",
    "total_val = len(normalized_mnist_val)\n",
    "\n",
    "with torch.no_grad(): # Forward propagation does not accumulate gradients\n",
    "    \n",
    "    for i, batch_train in enumerate(train_loader):\n",
    "        imgs_train, labels_train = batch_train\n",
    "        \n",
    "        outputs_train = model(imgs_train.view(batch_size, -1))\n",
    "        loss_train = loss_fn(outputs_train, labels_train)\n",
    "        loss_tensor_train[i] = loss_train.detach()\n",
    "        _, predicted_labels_train = torch.max(outputs_train, dim=1)\n",
    "        correct_train += int((predicted_labels_train == labels_train).sum())\n",
    "    \n",
    "    for i, batch_val in enumerate(val_loader):\n",
    "        imgs_val, labels_val = batch_val\n",
    "        \n",
    "        outputs_val = model(imgs_val.view(batch_size, -1))\n",
    "        loss_val = loss_fn(outputs_val, labels_val)\n",
    "        loss_tensor_val[i] = loss_val.detach()\n",
    "        _, predicted_labels_val = torch.max(outputs_val, dim=1)\n",
    "        correct_val += int((predicted_labels_val == labels_val).sum())\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"Training set accuracy: {:0.4f}\".format(correct_train/total_train),\n",
    "      \"Training set cost: {:0.4f}\".format(loss_tensor_train.mean(dim=0)),\n",
    "      \"Validation set accuracy: {:0.4f}\".format(correct_val/total_val),\n",
    "      \"Validation set cost: {:0.4f}\".format(loss_tensor_val.mean(dim=0)),\n",
    "      sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
